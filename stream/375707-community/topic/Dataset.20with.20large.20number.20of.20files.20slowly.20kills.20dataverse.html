<html>
<head><meta charset="utf-8"><title>Dataset with large number of files slowly kills dataverse Â· community Â· Zulip Chat Archive</title></head>
<h2>Stream: <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/index.html">community</a></h2>
<h3>Topic: <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/topic/Dataset.20with.20large.20number.20of.20files.20slowly.20kills.20dataverse.html">Dataset with large number of files slowly kills dataverse</a></h3>

<hr>

<base href="https://dataverse.zulipchat.com">

<head><link href="https://gdcc.github.io/zulip-archive/style.css" rel="stylesheet"></head>

<a name="479455004"></a>
<h4><a href="https://dataverse.zulipchat.com#narrow/stream/375707-community/topic/Dataset%20with%20large%20number%20of%20files%20slowly%20kills%20dataverse/near/479455004" class="zl"><img src="https://gdcc.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> PÃ©ter Pallinger <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/topic/Dataset.20with.20large.20number.20of.20files.20slowly.20kills.20dataverse.html#479455004">(Oct 29 2024 at 11:20)</a>:</h4>
<p>We have a dataset with a large number of files (about 450000). Loading this dataset takes about 30 seconds, and raises the (permanent) memory usage of payara by at least 1GB (based on the garbage collection logs). <br>
Is there a know mitigation for this? Some switch to disable file listing or similar?<br>
As it is now, about 20 loads of this dataset in the browser sends payara thrashing in GC hell. The only fix I know is restarting payara.</p>



<a name="479455314"></a>
<h4><a href="https://dataverse.zulipchat.com#narrow/stream/375707-community/topic/Dataset%20with%20large%20number%20of%20files%20slowly%20kills%20dataverse/near/479455314" class="zl"><img src="https://gdcc.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Philip Durbin ðŸš€ <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/topic/Dataset.20with.20large.20number.20of.20files.20slowly.20kills.20dataverse.html#479455314">(Oct 29 2024 at 11:22)</a>:</h4>
<p>Hmm, can you put those ~half a million files in a zip and use that instead as a new version of the dataset? We do have a nice zip previewer/downloader.</p>



<a name="479455423"></a>
<h4><a href="https://dataverse.zulipchat.com#narrow/stream/375707-community/topic/Dataset%20with%20large%20number%20of%20files%20slowly%20kills%20dataverse/near/479455423" class="zl"><img src="https://gdcc.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> PÃ©ter Pallinger <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/topic/Dataset.20with.20large.20number.20of.20files.20slowly.20kills.20dataverse.html#479455423">(Oct 29 2024 at 11:23)</a>:</h4>
<p>Possibly, of course. I will have to talk with the uploader. Is this the only way?</p>



<a name="479455630"></a>
<h4><a href="https://dataverse.zulipchat.com#narrow/stream/375707-community/topic/Dataset%20with%20large%20number%20of%20files%20slowly%20kills%20dataverse/near/479455630" class="zl"><img src="https://gdcc.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> PÃ©ter Pallinger <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/topic/Dataset.20with.20large.20number.20of.20files.20slowly.20kills.20dataverse.html#479455630">(Oct 29 2024 at 11:24)</a>:</h4>
<p>Of course, no single files could be download then. Or not even some smaller parts (sub-directories)...</p>



<a name="479458870"></a>
<h4><a href="https://dataverse.zulipchat.com#narrow/stream/375707-community/topic/Dataset%20with%20large%20number%20of%20files%20slowly%20kills%20dataverse/near/479458870" class="zl"><img src="https://gdcc.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Philip Durbin ðŸš€ <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/topic/Dataset.20with.20large.20number.20of.20files.20slowly.20kills.20dataverse.html#479458870">(Oct 29 2024 at 11:43)</a>:</h4>
<p>With the zip previewer/downloader, single files can be downloaded. You can try it on a zip file in my dataset if you like: <a href="https://dataverse.harvard.edu/file.xhtml?fileId=6867328&amp;version=4.0">https://dataverse.harvard.edu/file.xhtml?fileId=6867328&amp;version=4.0</a></p>



<a name="479459214"></a>
<h4><a href="https://dataverse.zulipchat.com#narrow/stream/375707-community/topic/Dataset%20with%20large%20number%20of%20files%20slowly%20kills%20dataverse/near/479459214" class="zl"><img src="https://gdcc.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> PÃ©ter Pallinger <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/topic/Dataset.20with.20large.20number.20of.20files.20slowly.20kills.20dataverse.html#479459214">(Oct 29 2024 at 11:45)</a>:</h4>
<p>Cool, I will look into this.</p>



<a name="479459487"></a>
<h4><a href="https://dataverse.zulipchat.com#narrow/stream/375707-community/topic/Dataset%20with%20large%20number%20of%20files%20slowly%20kills%20dataverse/near/479459487" class="zl"><img src="https://gdcc.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Philip Durbin ðŸš€ <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/topic/Dataset.20with.20large.20number.20of.20files.20slowly.20kills.20dataverse.html#479459487">(Oct 29 2024 at 11:46)</a>:</h4>
<p><a href="https://dataverse.harvard.edu/dataverse/ashkelonexcavations">https://dataverse.harvard.edu/dataverse/ashkelonexcavations</a> has 28K files. One dataset per file. Perhaps an extreme example but another way of avoiding having too many files in a single dataset.</p>



<a name="479470142"></a>
<h4><a href="https://dataverse.zulipchat.com#narrow/stream/375707-community/topic/Dataset%20with%20large%20number%20of%20files%20slowly%20kills%20dataverse/near/479470142" class="zl"><img src="https://gdcc.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> PÃ©ter Pallinger <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/topic/Dataset.20with.20large.20number.20of.20files.20slowly.20kills.20dataverse.html#479470142">(Oct 29 2024 at 12:43)</a>:</h4>
<p>Yeah, one file per dataset is not really a good solution IMHO. <br>
Also, the zip previewer needs to get the file list from the zip on the server, and parse it. With 450k files, it may crash the browser doing that...</p>



<a name="479470725"></a>
<h4><a href="https://dataverse.zulipchat.com#narrow/stream/375707-community/topic/Dataset%20with%20large%20number%20of%20files%20slowly%20kills%20dataverse/near/479470725" class="zl"><img src="https://gdcc.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> PÃ©ter Pallinger <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/topic/Dataset.20with.20large.20number.20of.20files.20slowly.20kills.20dataverse.html#479470725">(Oct 29 2024 at 12:46)</a>:</h4>
<p>Also, the "all files in a dataverse" approach makes it possible to search among them.</p>



<a name="479472434"></a>
<h4><a href="https://dataverse.zulipchat.com#narrow/stream/375707-community/topic/Dataset%20with%20large%20number%20of%20files%20slowly%20kills%20dataverse/near/479472434" class="zl"><img src="https://gdcc.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Philip Durbin ðŸš€ <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/topic/Dataset.20with.20large.20number.20of.20files.20slowly.20kills.20dataverse.html#479472434">(Oct 29 2024 at 12:55)</a>:</h4>
<p>Well, perhaps <span class="user-mention" data-user-id="652521">@Markus HaarlÃ¤nder</span>, author of the zip previewer/downloader, can confirm, but I believe it only downloads the bytes it needs to get the list.</p>



<a name="479475122"></a>
<h4><a href="https://dataverse.zulipchat.com#narrow/stream/375707-community/topic/Dataset%20with%20large%20number%20of%20files%20slowly%20kills%20dataverse/near/479475122" class="zl"><img src="https://gdcc.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> PÃ©ter Pallinger <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/topic/Dataset.20with.20large.20number.20of.20files.20slowly.20kills.20dataverse.html#479475122">(Oct 29 2024 at 13:09)</a>:</h4>
<p>Yes, it really only downloads the needed parts of the zip. However, representing a 450k long list in javascript and/or in the DOM is challenging for most browsers.<br>
Thank you for your help. I will try to convince the dataset owner to use a smaller number (~1000) of zip files, that way some of the search functionality remains but listing would be fast and would not leak (that much) memory.</p>



<a name="479480123"></a>
<h4><a href="https://dataverse.zulipchat.com#narrow/stream/375707-community/topic/Dataset%20with%20large%20number%20of%20files%20slowly%20kills%20dataverse/near/479480123" class="zl"><img src="https://gdcc.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Philip Durbin ðŸš€ <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/topic/Dataset.20with.20large.20number.20of.20files.20slowly.20kills.20dataverse.html#479480123">(Oct 29 2024 at 13:30)</a>:</h4>
<p>If it helps, <span class="user-mention" data-user-id="600327">@Ceilyn Boyd</span> gave a talk fairly recently called "Transforming a Digital Collection into a Data Collection". About 80K files were in play: <a href="https://groups.google.com/g/dataverse-community/c/Teb7_Pj2ajg/m/HO0E0vMnAQAJ">https://groups.google.com/g/dataverse-community/c/Teb7_Pj2ajg/m/HO0E0vMnAQAJ</a></p>



<a name="479522448"></a>
<h4><a href="https://dataverse.zulipchat.com#narrow/stream/375707-community/topic/Dataset%20with%20large%20number%20of%20files%20slowly%20kills%20dataverse/near/479522448" class="zl"><img src="https://gdcc.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> PÃ©ter Pallinger <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/topic/Dataset.20with.20large.20number.20of.20files.20slowly.20kills.20dataverse.html#479522448">(Oct 29 2024 at 16:40)</a>:</h4>
<p>If I upload a zip (even through the API), it will be decompressed. How can you upload a zip so that it is left as a zip?</p>



<a name="479523685"></a>
<h4><a href="https://dataverse.zulipchat.com#narrow/stream/375707-community/topic/Dataset%20with%20large%20number%20of%20files%20slowly%20kills%20dataverse/near/479523685" class="zl"><img src="https://gdcc.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Oliver Bertuch <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/topic/Dataset.20with.20large.20number.20of.20files.20slowly.20kills.20dataverse.html#479523685">(Oct 29 2024 at 16:47)</a>:</h4>
<p>Double-zip it <span aria-label="smiley" class="emoji emoji-1f603" role="img" title="smiley">:smiley:</span> (The official workaround)</p>



<a name="479532182"></a>
<h4><a href="https://dataverse.zulipchat.com#narrow/stream/375707-community/topic/Dataset%20with%20large%20number%20of%20files%20slowly%20kills%20dataverse/near/479532182" class="zl"><img src="https://gdcc.github.io/zulip-archive/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Philip Durbin ðŸš€ <a href="https://gdcc.github.io/zulip-archive/stream/375707-community/topic/Dataset.20with.20large.20number.20of.20files.20slowly.20kills.20dataverse.html#479532182">(Oct 29 2024 at 17:34)</a>:</h4>
<p>Yes, a please consider voting and commenting on this issue: Support uploading of archives (ZIP, other). <a href="https://github.com/IQSS/dataverse/issues/8029">#8029</a></p>



<hr><p>Last updated: Nov 01 2025 at 23:12 UTC</p>
</html>