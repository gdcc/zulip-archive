[
    {
        "content": "<p>I've set up an experimental stack on ODISSEI based on the development branch with our changes (xhtml files only). However, when we try to ingest via the Native API, I'm noticing this error: <code>Failed to export the dataset as ddi</code>. If you want, I can provide a complete dump from the logs. How can I best troubleshoot this?</p>",
        "id": 390320400,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694441761
    },
    {
        "content": "<p>Specifically: <code> Finalization: exception caught while exporting: Could not get prerequisite Optional[ddi] to create htmlexport for dataset 13\nio.gdcc.spi.export.ExportException: Could not get prerequisite Optional[ddi] to create htmlexport for dataset 13</code></p>",
        "id": 390320666,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694441836
    },
    {
        "content": "<p>Weird, it it ok to post here the file you're trying to ingest? (Or is it too big or restricted?)</p>",
        "id": 390322327,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1694442256
    },
    {
        "content": "<p>Totally fine, lemme ask the dev that's producing the file. I'll get one from the LISS metadata since that's open source (CBS is kinda restricted).</p>",
        "id": 390322776,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694442385
    },
    {
        "content": "<p>Great. Thanks. Also, did you branch before or after we released Dataverse 6.0 on Friday? Maybe you could just tell us the last commit from the main repo. Not your xhtml changes, I mean.</p>",
        "id": 390323399,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1694442542
    },
    {
        "content": "<p>This is the last commit I have:</p>\n<div class=\"codehilite\"><pre><span></span><code>commit f8bf5cb7e56150e6478b177085d4cf29d5142ca1\nMerge: 2246d66601 6f7f814ff0\nAuthor: landreev &lt;leonid@hmdc.harvard.edu&gt;\n</code></pre></div>",
        "id": 390324006,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694442709
    },
    {
        "content": "<p>Looks like I don't, looking at commits. Shall I update?</p>",
        "id": 390324952,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694442949
    },
    {
        "content": "<p>(In fact; I might just pin on the 6.0 tag)</p>",
        "id": 390325072,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694442967
    },
    {
        "content": "<p><a href=\"https://github.com/IQSS/dataverse/commit/f8bf5cb7e56150e6478b177085d4cf29d5142ca1\">https://github.com/IQSS/dataverse/commit/f8bf5cb7e56150e6478b177085d4cf29d5142ca1</a> great, thanks. I have a better sense of where you are. <span aria-label=\"big smile\" class=\"emoji emoji-1f604\" role=\"img\" title=\"big smile\">:big_smile:</span> No need to update.</p>",
        "id": 390325219,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1694443009
    },
    {
        "content": "<p>You're almost on 6.0.</p>",
        "id": 390327076,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1694443468
    },
    {
        "content": "<p>If you want, I can pin there in case you want usable feedback specifically on that version )</p>",
        "id": 390334466,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694445402
    },
    {
        "content": "<p>No, no, if anything I'd probably ask you to pin to alpha/master.</p>",
        "id": 390334898,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1694445527
    },
    {
        "content": "<p>I pinned to development for now, with all due side effects of that (but can pin to master if you'd prefer). Open to structurally stable suggestions : )</p>",
        "id": 390338628,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694446514
    },
    {
        "content": "<p>The develop branch is fine but previously you were an alpha/master guy. <span aria-label=\"big smile\" class=\"emoji emoji-1f604\" role=\"img\" title=\"big smile\">:big_smile:</span></p>",
        "id": 390339063,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1694446650
    },
    {
        "content": "<p>Oh, I'm running the alpha image. That hasn't changed.</p>",
        "id": 390340800,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694447205
    },
    {
        "content": "<p>It's like making beer; you need a stable base to experiment from.</p>",
        "id": 390340878,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694447248
    },
    {
        "content": "<p>Currently that base is open to change ; ) Everything is, until it gets users.</p>",
        "id": 390340927,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694447268
    },
    {
        "content": "<p>Are we sure this is container related? It sounds a bit like a general issue. Could this be tested with <a href=\"http://demo.dataverse.org\">demo.dataverse.org</a>? Should we move this to <a class=\"stream\" data-stream-id=\"379673\" href=\"/#narrow/stream/379673-dev\">#dev</a> ?</p>",
        "id": 390346209,
        "sender_full_name": "Oliver Bertuch",
        "timestamp": 1694448857
    },
    {
        "content": "<p>Actually, I have some idea all of a sudden I'll tell our dev first.</p>",
        "id": 390351376,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694450841
    },
    {
        "content": "<p>Small explanation; I noticed collision in field names in our tsv files between core and our files; I renamed some of them and reset everything to work with new files. Could be it's breaking there.</p>",
        "id": 390351609,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694450916
    },
    {
        "content": "<p>The dev in question uses the Native Api; quite possible that just validates schema format and the error is served later in the chain when it tries to produce a ddi file, assuming nested key/value pairs are as schema demands but in fact are different. I'd still expect that to be validated earlier, but it's a bit of an edge case.</p>",
        "id": 390353267,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694451568
    },
    {
        "content": "<p>This topic was moved here from <a class=\"stream-topic\" data-stream-id=\"375812\" href=\"/#narrow/stream/375812-containers/topic/Error.20while.20ingesting.20records\">#containers &gt; Error while ingesting records</a> by <span class=\"user-mention silent\" data-user-id=\"598112\">Philip Durbin</span>.</p>",
        "id": 390375808,
        "sender_full_name": "Notification Bot",
        "timestamp": 1694460890
    },
    {
        "content": "<p>Sounds plausible.</p>",
        "id": 390375891,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1694460935
    },
    {
        "content": "<p>I did go ahead and move this to troubleshooting.</p>",
        "id": 390375914,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1694460949
    },
    {
        "content": "<p>Good one; We're trying with a dataset that doesn't hit that specific metadata block. Easy way to check.</p>",
        "id": 390463825,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694505117
    },
    {
        "content": "<p>Still the same problem, so much for eleventh hour Ballmer peak insight. I've gotten a record which we've tried to ingest using the Native API, attached here.<br>\n<a href=\"/user_uploads/53090/ol5yZ2Tdb7P2ECtEu6ZUM_0B/dataverse_metadata_ingest_example.json\">dataverse_metadata_ingest_example.json</a></p>",
        "id": 390468310,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694506815
    },
    {
        "content": "<p>I see a bunch of DANS-specific metadata: dansRightsHolder, dansPersonalDataPresent, dansAudience, dansCollection, dansTemporalCoverage, dansDataversePid, dansDataversePidVersion, dansBagId, dansNbn</p>",
        "id": 390496541,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1694517235
    },
    {
        "content": "<p>But I'm not sure why that would affect DDI export.</p>",
        "id": 390496605,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1694517248
    },
    {
        "content": "<p>Yeap, that's correct; we juggle a fair amount of metadata blocks.</p>",
        "id": 390499400,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694518376
    },
    {
        "content": "<p>If you remove the DANS metadata blocks from the JSON, do you still get the DDI error?</p>",
        "id": 390503920,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1694520036
    },
    {
        "content": "<p>Sorry for the delay here (had a mix of vacation days to deal with); yeah, we can confirm this is still the case. I just reset the whole stack, removed the metadata blocks by tossing the DB volume, and tried a vanilla upload. However, I note that:</p>\n<ul>\n<li>Datasets do appear in the interface now after I dropped all metadata blocks. Next step is start turning them on/off again one by one to find out the issue.</li>\n<li>The export to DDI is broken, but this is because it uses a hostname that doesn't resolve (<code>localhost</code> on a host with full FQDN). Is there an env /docker-compose  environment setting I can use for this?</li>\n</ul>",
        "id": 390918836,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694694423
    },
    {
        "content": "<p>DDI export relies on the hostname?</p>",
        "id": 390926183,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1694696753
    },
    {
        "content": "<p>It calls the API, so practically yes. If I rewrite the correct path manually, it simply \"fails\" without further comment.</p>",
        "id": 390926858,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694696934
    },
    {
        "content": "<p>(I'm assuming DDI files are statically generated; if so, that would make sense that it'd fail if the file couldn't be generated earlier)</p>",
        "id": 390928261,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1694697400
    },
    {
        "content": "<p>Would you be able to open an issue about this?</p>",
        "id": 390929232,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1694697695
    },
    {
        "content": "<p>Yes, certainly. I can provide an env for this as well, plus credentials.</p>",
        "id": 392471079,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1695372732
    },
    {
        "content": "<p>I forgot to update this topic; but it works (I think). At least it does manually.</p>",
        "id": 396273090,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1697111882
    },
    {
        "content": "<p>Can be resolved.</p>",
        "id": 396273174,
        "sender_full_name": "Thomas van Erven",
        "timestamp": 1697111903
    },
    {
        "content": "<p>Great!</p>",
        "id": 396277470,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1697113452
    },
    {
        "content": "<p>Is there anything we should document? If so, please feel free to great an issue.</p>",
        "id": 396277525,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1697113478
    }
]