[
    {
        "content": "<p>python-dvuploader works quite nice now I can test direct upload on <a href=\"http://demo.dataverse.org\">demo.dataverse.org</a>. I was wondering if it possible to tune the upload parameters, like when you have a lot of small files of a few larger ones. <span class=\"user-mention\" data-user-id=\"599841\">@Jan Range</span>, what is you experience?  For example, my experience with s5cmd is that setting the number of workers and concurrency can improve the transfer speed: <a href=\"https://github.com/peak/s5cmd?tab=readme-ov-file#configuring-concurrency\">https://github.com/peak/s5cmd?tab=readme-ov-file#configuring-concurrency</a> In my case I get 500 MB/s or higher with s5cmd to another s3 bucket, but with dvuploader it is around 80 MB/s, even when I increase the number of jobs. The number of jobs seems to not have much effect. Not sure if this is expected. Happy to hear what others do and get in terms of large data transfers.</p>",
        "id": 503566053,
        "sender_full_name": "Mattias de Hollander",
        "timestamp": 1741191988
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"883325\">Mattias de Hollander</span> <a href=\"#narrow/channel/432390-large-data/topic/setting.20parameters.20for.20direct.20upload.20with.20python-dvuploader/near/503566053\">said</a>:</p>\n<blockquote>\n<p>python-dvuploader works quite nice now I can test direct upload on <a href=\"http://demo.dataverse.org\">demo.dataverse.org</a>. I was wondering if it possible to tune the upload parameters, like when you have a lot of small files of a few larger ones. <span class=\"user-mention silent\" data-user-id=\"599841\">Jan Range</span>, what is you experience?  For example, my experience with s5cmd is that setting the number of workers and concurrency can improve the transfer speed: <a href=\"https://github.com/peak/s5cmd?tab=readme-ov-file#configuring-concurrency\">https://github.com/peak/s5cmd?tab=readme-ov-file#configuring-concurrency</a> In my case I get 500 MB/s or higher with s5cmd to another s3 bucket, but with dvuploader it is around 80 MB/s, even when I increase the number of jobs. The number of jobs seems to not have much effect. Not sure if this is expected. Happy to hear what others do and get in terms of large data transfers.</p>\n</blockquote>\n<p>Just a friendly reminder about my previous question on optimizing upload speeds with dvuploader. I'm still curious to know if it can utilize parallel threads to boost speeds like s5cmd does - I've noticed s5cmd (written in Go) can reach 500 MB/s or higher, while dvuploader (in Python) tops out at around 80 MB/s for me. Is this expected, or is there a way to tweak dvuploader to get closer to maxing out my network bandwidth?</p>",
        "id": 504857939,
        "sender_full_name": "Mattias de Hollander",
        "timestamp": 1741703365
    },
    {
        "content": "<p>I'm not sure but I moved the somewhat off topic posts to their own thread.</p>",
        "id": 504859267,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1741703625
    }
]