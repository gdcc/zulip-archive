[
    {
        "content": "<p>When we released 6.3 there were some failing tests reported by api-test-runner for the updated \"alpha\" image: <a href=\"https://github.com/gdcc/api-test-runner/actions/runs/9775864881\">https://github.com/gdcc/api-test-runner/actions/runs/9775864881</a></p>\n<p>I didn't worry too much about it because test were passing in Jenkins.</p>\n<p>That said, it would be great to have the \"alpha\" workflow show green again, like it did for previous releases.</p>",
        "id": 451512964,
        "sender_full_name": "Philip Durbin 🚀",
        "timestamp": 1721053494
    },
    {
        "content": "<p>Looks like timing related things to me. I've said it before, let me say it again: the way we setup and run our E2E tests is flawed to be suffering from timing issues.</p>",
        "id": 451514287,
        "sender_full_name": "Oliver Bertuch",
        "timestamp": 1721053826
    },
    {
        "content": "<p>Should we create an issue?</p>",
        "id": 451514493,
        "sender_full_name": "Philip Durbin 🚀",
        "timestamp": 1721053879
    },
    {
        "content": "<p>Dunno. This is not a simple thing. I suspect it will require restructuring these tests and that is a lot of work. I sincerely doubt anyway wants to put these hours in.</p>",
        "id": 451514661,
        "sender_full_name": "Oliver Bertuch",
        "timestamp": 1721053924
    },
    {
        "content": "<p>But we all want passing tests, right? <span aria-label=\"grinning\" class=\"emoji emoji-1f600\" role=\"img\" title=\"grinning\">:grinning:</span> Especially at release time.</p>",
        "id": 451514749,
        "sender_full_name": "Philip Durbin 🚀",
        "timestamp": 1721053954
    },
    {
        "content": "<p>Right now we call this step of checking the \"alpha\" image optional: <a href=\"https://guides.dataverse.org/en/6.3/developers/making-releases.html#optional-test-docker-images\">https://guides.dataverse.org/en/6.3/developers/making-releases.html#optional-test-docker-images</a></p>",
        "id": 451514909,
        "sender_full_name": "Philip Durbin 🚀",
        "timestamp": 1721054003
    },
    {
        "content": "<p>Well what we should take a look at, if we really want to change things: don't create everything over and over again for tests and just assume it will be done in time. We put a lot of stress on the test runner and waste CPU cycles to re-create things like collections, users and datasets. And then we often just wait for a few seconds, hope all is good and light the fuse on the test canon...</p>",
        "id": 451516046,
        "sender_full_name": "Oliver Bertuch",
        "timestamp": 1721054235
    },
    {
        "content": "<p>Sounds like good advice.</p>",
        "id": 451516374,
        "sender_full_name": "Philip Durbin 🚀",
        "timestamp": 1721054309
    },
    {
        "content": "<p>Right now it says this:</p>\n<p>\"If there are failures, additional dependencies or settings may have been added to the “develop” workflow. Copy them over and try again.\"</p>",
        "id": 451520852,
        "sender_full_name": "Philip Durbin 🚀",
        "timestamp": 1721055537
    },
    {
        "content": "<p>But what if you try again and again and it keeps failing? That's where I was during release. Probably an issue should be created, right?</p>",
        "id": 451525294,
        "sender_full_name": "Philip Durbin 🚀",
        "timestamp": 1721056746
    }
]