[
    {
        "content": "<p>There's a nice \"direct upload to S3\" script at <a href=\"https://github.com/IQSS/dataverse.harvard.edu/tree/3fc9bfe9a171b2f7546ad44b1114f5c3920907d1/util/python/direct-upload\">https://github.com/IQSS/dataverse.harvard.edu/tree/3fc9bfe9a171b2f7546ad44b1114f5c3920907d1/util/python/direct-upload</a></p>\n<p><span class=\"user-mention\" data-user-id=\"599841\">@Jan Range</span> how do you feel about adding it to easyDataverse or pyDataverse?</p>\n<p>I just checked with Leonid and he's cool with it. It should like you two even discussed it already. The only caveat, he said, is that it doesn't support multipart S3 upload. This is mentioned already in the README. ^^</p>",
        "id": 342169469,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1678915450
    },
    {
        "content": "<p>Yes, already put this in my ToDo's for this and next week. Does Demo Dataverse allow direct uploads already?</p>",
        "id": 342171800,
        "sender_full_name": "Jan Range",
        "timestamp": 1678916380
    },
    {
        "content": "<p>Yes, it does. But if you have any trouble at all, please ping me!</p>",
        "id": 342174737,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1678917498
    },
    {
        "content": "<p>Should I create the issue in the easyDataverse repo or the pyDataverse repo?</p>",
        "id": 342174768,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1678917518
    },
    {
        "content": "<p>Lets go for pyDataverse, I think its best suited for</p>",
        "id": 342177852,
        "sender_full_name": "Jan Range",
        "timestamp": 1678918755
    },
    {
        "content": "<p>Ah, the person who wants this gave it a <span aria-label=\"tada\" class=\"emoji emoji-1f389\" role=\"img\" title=\"tada\">:tada:</span> <span aria-label=\"smile\" class=\"emoji emoji-1f642\" role=\"img\" title=\"smile\">:smile:</span></p>",
        "id": 342282046,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1678966003
    },
    {
        "content": "<p>Perfect <span aria-label=\"raised hands\" class=\"emoji emoji-1f64c\" role=\"img\" title=\"raised hands\">:raised_hands:</span> In this case we may not need to create a new issue as it has already been pointed out in <a href=\"https://github.com/gdcc/pyDataverse/issues/136\">#136</a></p>",
        "id": 342282702,
        "sender_full_name": "Jan Range",
        "timestamp": 1678966191
    },
    {
        "content": "<p>Just added this issue: <a href=\"https://github.com/gdcc/pyDataverse/issues/157\">https://github.com/gdcc/pyDataverse/issues/157</a></p>",
        "id": 342316820,
        "sender_full_name": "Ceilyn Boyd",
        "timestamp": 1678974514
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"599841\">@Jan Range</span> as Phil says, <a href=\"http://demo.dataverse.org\">demo.dataverse.org</a> does support direct upload, but I don't believe that's the \"default\" datastore. be certain to ask (Kevin or Leonid) which datastore your collection is using - only cautioning as I don't believe you get direct S3 upload by default.</p>",
        "id": 343342131,
        "sender_full_name": "Don Sizemore",
        "timestamp": 1679397938
    },
    {
        "content": "<p>Yes, Leonid emphasized we should let him know if direct upload isn't working on demo.</p>",
        "id": 343342657,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1679398086
    },
    {
        "content": "<p>Alright, perfect! Thanks for the heads up :-)</p>",
        "id": 343348666,
        "sender_full_name": "Jan Range",
        "timestamp": 1679399803
    },
    {
        "content": "<p>Just tried it and I dont have permission for a direct upload <span aria-label=\"sweat\" class=\"emoji emoji-1f613\" role=\"img\" title=\"sweat\">:sweat:</span> Can I just send you the name of my collection or can you set one up for me on Demo Dataverse?</p>",
        "id": 343791016,
        "sender_full_name": "Jan Range",
        "timestamp": 1679521218
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"599841\">@Jan Range</span> when you get a chance, can you please ask Leonid on Slack?</p>",
        "id": 343795514,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1679523271
    },
    {
        "content": "<p>Alright, will do :-)</p>",
        "id": 343796148,
        "sender_full_name": "Jan Range",
        "timestamp": 1679523553
    },
    {
        "content": "<p>Got it <span aria-label=\"tada\" class=\"emoji emoji-1f389\" role=\"img\" title=\"tada\">:tada:</span></p>",
        "id": 343796917,
        "sender_full_name": "Jan Range",
        "timestamp": 1679523924
    },
    {
        "content": "<p>So far everything is working, will transfer the code to pyDataverse and open a pull request once tests are implemented. One thing I wanted to ask is if there is a functionality to assemble chunks at Dataverse. Maybe this way we can override the max_part_size of AWS</p>",
        "id": 343937357,
        "sender_full_name": "Jan Range",
        "timestamp": 1679568538
    },
    {
        "content": "<p>Uh. I'm probably misunderstanding the question (let me know if you'd like to hop on a video call) but DVUploaders has a way to to break up files client-side, if that helps: <a href=\"https://github.com/GlobalDataverseCommunityConsortium/dataverse-uploader/blob/v1.1.0/src/main/java/org/sead/uploader/dataverse/HttpPartUploadJob.java#L25\">https://github.com/GlobalDataverseCommunityConsortium/dataverse-uploader/blob/v1.1.0/src/main/java/org/sead/uploader/dataverse/HttpPartUploadJob.java#L25</a></p>",
        "id": 343977510,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1679577666
    },
    {
        "content": "<p>Perfect, that was what I was looking for!</p>",
        "id": 343985959,
        "sender_full_name": "Jan Range",
        "timestamp": 1679579626
    },
    {
        "content": "<p>Happy to update you that the S3 upload is implemented into Python now. Will be shipped with the next EasyDataverse update!</p>",
        "id": 391623552,
        "sender_full_name": "Jan Range",
        "timestamp": 1695015822
    },
    {
        "content": "<p>I have a question though, the multipart upload is parallelized and I have yet only tested this with rather small files. However, I am certain that big files might cause memory/bandwith issues when uploaded in parallel. Hence, I'd like to restrict the number or size of part uploads. </p>\n<p>This may depend on the users machine/connection and will be customizable, but do you have any suggestion of a default? Like 4gb maximum upload volume at once?</p>",
        "id": 391623820,
        "sender_full_name": "Jan Range",
        "timestamp": 1695016049
    },
    {
        "content": "<p>Btw if you are interested, here is the branch of the next ED release. Once tests are up and running, it'll be shipped .</p>\n<p><a href=\"https://github.com/gdcc/easyDataverse/tree/flexible-connect\">https://github.com/gdcc/easyDataverse/tree/flexible-connect</a></p>",
        "id": 391624426,
        "sender_full_name": "Jan Range",
        "timestamp": 1695016471
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"637063\">@Leo Andreev</span> check it out! S3 upload in EasyDataverse! ^^ <span aria-label=\"tada\" class=\"emoji emoji-1f389\" role=\"img\" title=\"tada\">:tada:</span></p>",
        "id": 391715127,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1695048032
    },
    {
        "content": "<p>Hi All, </p>\n<p>I've uploaded 180 GB files to  S3 (multipart of course), with my own script, and the main problems I've noticed are: </p>\n<ul>\n<li>when you open a file to read (even part files), it's stored in memory, and you have to take care of memory management on your own (ex. uploading 20 GB file will fail on most laptops if you want clean your mem) </li>\n<li>update links by default have 60 min lifetime (it might be too short in case bad network connection for large files (&gt;32gb))</li>\n<li>after successfully uploading, publishing large files might cause problems during calculating checksum ( it takes too long and publishing fails ) </li>\n</ul>\n<p>I know, that 180GB is extreme, but still for 32GB+ files those problems will occur. I put it in this thread because direct upload to S3 is in fact the only way to deal with large files.</p>",
        "id": 395855170,
        "sender_full_name": "Andrzej Zemla",
        "timestamp": 1696934781
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"657879\">@Andrzej Zemla</span> hi! Have you tried <a href=\"https://github.com/gdcc/python-dvuploader\">https://github.com/gdcc/python-dvuploader</a> by <span class=\"user-mention\" data-user-id=\"599841\">@Jan Range</span> ? It's new! Announced here: <a href=\"https://groups.google.com/g/dataverse-community/c/TQZJOpYmXbU/m/6m27nm4dAQAJ\">https://groups.google.com/g/dataverse-community/c/TQZJOpYmXbU/m/6m27nm4dAQAJ</a></p>",
        "id": 395858463,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1696936020
    },
    {
        "content": "<p>No i didn't, I needed it in July ;), but I'll test it for sure, and write you a feedback</p>",
        "id": 395868805,
        "sender_full_name": "Andrzej Zemla",
        "timestamp": 1696939773
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"657879\">@Andrzej Zemla</span> thanks, that would be extremely helpful <span aria-label=\"blush\" class=\"emoji emoji-1f60a\" role=\"img\" title=\"blush\">:blush:</span></p>",
        "id": 395877286,
        "sender_full_name": "Jan Range",
        "timestamp": 1696942424
    }
]