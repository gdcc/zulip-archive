[
    {
        "content": "<p>We have a dataset with a large number of files (about 450000). Loading this dataset takes about 30 seconds, and raises the (permanent) memory usage of payara by at least 1GB (based on the garbage collection logs). <br>\nIs there a know mitigation for this? Some switch to disable file listing or similar?<br>\nAs it is now, about 20 loads of this dataset in the browser sends payara thrashing in GC hell. The only fix I know is restarting payara.</p>",
        "id": 479455004,
        "sender_full_name": "PÃ©ter Pallinger",
        "timestamp": 1730200845
    },
    {
        "content": "<p>Hmm, can you put those ~half a million files in a zip and use that instead as a new version of the dataset? We do have a nice zip previewer/downloader.</p>",
        "id": 479455314,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1730200954
    },
    {
        "content": "<p>Possibly, of course. I will have to talk with the uploader. Is this the only way?</p>",
        "id": 479455423,
        "sender_full_name": "PÃ©ter Pallinger",
        "timestamp": 1730201001
    },
    {
        "content": "<p>Of course, no single files could be download then. Or not even some smaller parts (sub-directories)...</p>",
        "id": 479455630,
        "sender_full_name": "PÃ©ter Pallinger",
        "timestamp": 1730201069
    },
    {
        "content": "<p>With the zip previewer/downloader, single files can be downloaded. You can try it on a zip file in my dataset if you like: <a href=\"https://dataverse.harvard.edu/file.xhtml?fileId=6867328&amp;version=4.0\">https://dataverse.harvard.edu/file.xhtml?fileId=6867328&amp;version=4.0</a></p>",
        "id": 479458870,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1730202203
    },
    {
        "content": "<p>Cool, I will look into this.</p>",
        "id": 479459214,
        "sender_full_name": "PÃ©ter Pallinger",
        "timestamp": 1730202312
    },
    {
        "content": "<p><a href=\"https://dataverse.harvard.edu/dataverse/ashkelonexcavations\">https://dataverse.harvard.edu/dataverse/ashkelonexcavations</a> has 28K files. One dataset per file. Perhaps an extreme example but another way of avoiding having too many files in a single dataset.</p>",
        "id": 479459487,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1730202407
    },
    {
        "content": "<p>Yeah, one file per dataset is not really a good solution IMHO. <br>\nAlso, the zip previewer needs to get the file list from the zip on the server, and parse it. With 450k files, it may crash the browser doing that...</p>",
        "id": 479470142,
        "sender_full_name": "PÃ©ter Pallinger",
        "timestamp": 1730205815
    },
    {
        "content": "<p>Also, the \"all files in a dataverse\" approach makes it possible to search among them.</p>",
        "id": 479470725,
        "sender_full_name": "PÃ©ter Pallinger",
        "timestamp": 1730205980
    },
    {
        "content": "<p>Well, perhaps <span class=\"user-mention\" data-user-id=\"652521\">@Markus HaarlÃ¤nder</span>, author of the zip previewer/downloader, can confirm, but I believe it only downloads the bytes it needs to get the list.</p>",
        "id": 479472434,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1730206527
    },
    {
        "content": "<p>Yes, it really only downloads the needed parts of the zip. However, representing a 450k long list in javascript and/or in the DOM is challenging for most browsers.<br>\nThank you for your help. I will try to convince the dataset owner to use a smaller number (~1000) of zip files, that way some of the search functionality remains but listing would be fast and would not leak (that much) memory.</p>",
        "id": 479475122,
        "sender_full_name": "PÃ©ter Pallinger",
        "timestamp": 1730207378
    },
    {
        "content": "<p>If it helps, <span class=\"user-mention\" data-user-id=\"600327\">@Ceilyn Boyd</span> gave a talk fairly recently called \"Transforming a Digital Collection into a Data Collection\". About 80K files were in play: <a href=\"https://groups.google.com/g/dataverse-community/c/Teb7_Pj2ajg/m/HO0E0vMnAQAJ\">https://groups.google.com/g/dataverse-community/c/Teb7_Pj2ajg/m/HO0E0vMnAQAJ</a></p>",
        "id": 479480123,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1730208642
    },
    {
        "content": "<p>If I upload a zip (even through the API), it will be decompressed. How can you upload a zip so that it is left as a zip?</p>",
        "id": 479522448,
        "sender_full_name": "PÃ©ter Pallinger",
        "timestamp": 1730220039
    },
    {
        "content": "<p>Double-zip it <span aria-label=\"smiley\" class=\"emoji emoji-1f603\" role=\"img\" title=\"smiley\">:smiley:</span> (The official workaround)</p>",
        "id": 479523685,
        "sender_full_name": "Oliver Bertuch",
        "timestamp": 1730220429
    },
    {
        "content": "<p>Yes, a please consider voting and commenting on this issue: Support uploading of archives (ZIP, other). <a href=\"https://github.com/IQSS/dataverse/issues/8029\">#8029</a></p>",
        "id": 479532182,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1730223258
    }
]