[
    {
        "content": "<p>Good morning! I have deployed a test instance of Dataverse for the Barcelona Super Computer center. Some users are asking about the best way to upload data. For large files is there a recommended API?  (Users are asking if PUT can be used instead of post in order to allow chunking of the transfer. I havent found much about it in the documentation).  Am I missing another way of chunking large files ? Also for parrelel transfers are there any recomendations ?</p>",
        "id": 455869505,
        "sender_full_name": "Simon Carroll",
        "timestamp": 1722586869
    },
    {
        "content": "<p>Hi! Are you using S3-compatible storage? Do you have \"direct upload\" enabled? If so, I'd recommend trying DVUploader: <a href=\"https://guides.dataverse.org/en/6.3/user/dataset-management.html#command-line-dvuploader\">https://guides.dataverse.org/en/6.3/user/dataset-management.html#command-line-dvuploader</a></p>",
        "id": 455915345,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1722599279
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"598112\">Philip Durbin</span> <a href=\"#narrow/stream/375707-community/topic/Data.20upload.20questions/near/455915345\">said</a>:</p>\n<blockquote>\n<p>Hi! Are you using S3-compatible storage? Do you have \"direct upload\" enabled? If so, I'd recommend trying DVUploader: <a href=\"https://guides.dataverse.org/en/6.3/user/dataset-management.html#command-line-dvuploader\">https://guides.dataverse.org/en/6.3/user/dataset-management.html#command-line-dvuploader</a></p>\n</blockquote>\n<p>Hello! Thanks for the reply. At the moment no. We will potentially be using swift in the future. For the moment it is just an openstack storage volume mounted on the VM. Later it could be some combination of swift with our ibm spectrum archive storage system.</p>",
        "id": 455926231,
        "sender_full_name": "Simon Carroll",
        "timestamp": 1722602908
    },
    {
        "content": "<p>Do you have anny recommendations for best way to upload files in the current architecture ?</p>",
        "id": 455926343,
        "sender_full_name": "Simon Carroll",
        "timestamp": 1722602937
    },
    {
        "content": "<p>Ok, so no S3. No Globus either, I assume: <a href=\"https://guides.dataverse.org/en/6.3/admin/integrations.html#globus\">https://guides.dataverse.org/en/6.3/admin/integrations.html#globus</a></p>",
        "id": 455928871,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1722603745
    },
    {
        "content": "<p>For a one off large file upload to non-S3, I've been chatting with <span class=\"user-mention\" data-user-id=\"735407\">@MarÃ­a A. Matienzo</span> about a way to do it over at <a class=\"stream-topic\" data-stream-id=\"378866\" href=\"/#narrow/stream/378866-troubleshooting/topic/large.20uploads.20via.20native.20api.20workaround\">#troubleshooting &gt; large uploads via native api workaround</a></p>",
        "id": 455929001,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1722603809
    },
    {
        "content": "<p>As <span class=\"user-mention\" data-user-id=\"626369\">@Don Sizemore</span> reminded me, there's also the concept of Trusted Remote Storage: <a href=\"https://guides.dataverse.org/en/6.3/installation/config.html#trusted-remote-storage\">https://guides.dataverse.org/en/6.3/installation/config.html#trusted-remote-storage</a></p>\n<p>In short, Dataverse doesn't manage the files. You just tell Dataverse where the files live.</p>",
        "id": 455930261,
        "sender_full_name": "Philip Durbin ðŸš€",
        "timestamp": 1722604297
    }
]